
# ğŸ§  Customer Churn Prediction (Dockerized ETL + Streamlit App)

This project is a fully containerized data pipeline built using **Docker Compose**. It automates the process of:
- Setting up a PostgreSQL database
- Creating the required schema
- Loading and cleaning customer churn data
- Launching an interactive **Streamlit** dashboard

> âš ï¸ The project assumes that the required files (`customer_churn_data.csv`, Dockerfiles, and Python scripts) are already present in their respective directories and will not suggest any file changes.

---

## ğŸ“ Folder Structure

```
customer-churn-project/
â”‚
â”œâ”€â”€ docker-compose.yml                   # Orchestrates all services
â”œâ”€â”€ customer_churn_data.csv              # Raw churn data used in the ETL process
â”‚
â”œâ”€â”€ pg_db_creation/                      # Initializes PostgreSQL schema
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pg_db_creation.py
â”‚
â”œâ”€â”€ data_load/                           # Loads CSV into the PostgreSQL database
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data_load.py
â”‚
â”œâ”€â”€ data_cleaning/                       # Cleans and processes data
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data_cleaning.py
â”‚
â””â”€â”€ app/                                 # Streamlit app for churn data visualization
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ app.py
```

---

## ğŸš€ Getting Started

### âœ… Prerequisites

Ensure you have the following installed:
- Docker: https://docs.docker.com/get-docker/
- Docker Compose: https://docs.docker.com/compose/install/

### ğŸ“¦ Clone the Repository

```bash
git clone https://github.com/<your-username>/customer-churn-project.git
cd customer-churn-project
```

---

## ğŸ›  Running the Application

### ğŸ”§ Step 1: Build and Start Containers

```bash
docker-compose up --build
```

This command will:

1. Start a PostgreSQL database container
2. Build and run scripts to:
   - Create database schema
   - Load CSV data into the database
   - Clean and preprocess the data
3. Launch the Streamlit dashboard on port `8501`

### ğŸŒ Step 2: Access the App

Open your browser and go to:

```
http://localhost:8501
```

You will see the Streamlit dashboard that visualizes cleaned churn data from PostgreSQL.

---

## ğŸ“Œ Services Overview

| Service         | Description                                 | Port   |
|----------------|---------------------------------------------|--------|
| `db`           | PostgreSQL database (containerized)         | 5431   |
| `pg_db_creation` | Creates required schema in the database   | N/A    |
| `data_load`    | Loads CSV data into the database             | N/A    |
| `data_cleaning`| Applies data cleaning and preprocessing      | N/A    |
| `app`          | Streamlit dashboard                          | 8501   |

---

## ğŸ’¾ Data Persistence

PostgreSQL uses a Docker-managed named volume `pgdata` to persist data across container restarts.

---

## â“ FAQs

### Where should I place the CSV file?

Place the `customer_churn_data.csv` file at the **root level** of the project â€” the same level as `docker-compose.yml`.

### Can I use a different port?

Yes. You can modify the exposed ports in `docker-compose.yml`.

### I got a `file not found` error?

Ensure that:
- You are inside the root directory of the project when running `docker-compose up`
- All necessary files (CSV, Python scripts, Dockerfiles) exist in their respective folders

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE). You are free to use, modify, and distribute it with attribution.

---

## ğŸ™‹â€â™‚ï¸ Questions / Contributions

If you encounter issues or have feature suggestions, feel free to:
- Raise an issue
- Fork the repo and open a pull request

---

**Built with â¤ï¸ using Python, Docker, PostgreSQL, and Streamlit.**
